{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we import necessary packages \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import functions\n",
    "import twitter\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static variable saves how many times the program has been run \n",
    "run_var = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Status                                              Tweet\n",
      "0     bot    #ThingsDoneByMistake kissing auntie in the lips\n",
      "1     bot  RT @mc_derpin: #TheOlderWeGet the more pessimi...\n",
      "2     bot  RT @dmataconis: Ready To Feel Like A Failure? ...\n",
      "3     bot    Amen! #blacklivesmatter https://t.co/wGffaOqgzl\n",
      "4     bot  RT @NahBabyNah: Twitchy: Chuck Todd caught out...\n",
      "5     bot  RT @mcicero10: #BernieSanders #Trump people sh...\n",
      "6     bot  RT @ItsJustJaynie: @HillaryClinton The undecid...\n",
      "7     bot                           @TodayCleveland 'no way'\n",
      "8     bot  @NickTomaWBRE Hi, Nick! We're holding a \"Miner...\n",
      "9     bot  What. Is. A. Resolution #My4WordNewYearsResolu...\n",
      "10    bot  Lifetime movie   your pet   psycho neighbor = ...\n",
      "11    bot  RT @Conservatexian: New post: \"UN alarm that m...\n",
      "12    bot  RT @HillaryClinton: This one's for you, Hillar...\n",
      "13    bot  RT @leonpui_: Hillary Clinton, Obama and the D...\n",
      "14    bot  5th grade. When the second plane hit, I though...\n",
      "15    bot  #teapartynews #teaparty #theteaparty #politics...\n",
      "16    bot  #TrumpBecause #DonaldTrump will not be bought!...\n",
      "17    bot  RT @DMashak: #Debates Chris Wallace @FoxNewsSu...\n",
      "18    bot  RT @mikefdupjourney: @she_nutt You're welcome!...\n",
      "19    bot  RT @PrisonPlanet: Hillary's anti-Trump poster ...\n"
     ]
    }
   ],
   "source": [
    "text_data = pd.read_csv('tweets.csv')\n",
    "df = pd.DataFrame({\"Tweet\": text_data['text'][0:20], \"Status\": text_data['category'][0:20]})\n",
    "print(df)\n",
    "\n",
    "#df = pd.DataFrame({\"Time Str\": text_data['created_str'], \"Time\": text_data['created_at'], \"Tweet\": text_data['text'], \"Category\": text_data['category']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sort_values(by=['Time'])\n",
    "df = df.dropna(axis=0, how='all')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = pd.DataFrame({\"Date\": df['Time Str'], \"Time\": df['Time'], \"Tweet\": df['Tweet']})\n",
    "#s = functions.polarizeFrame(f['Date'], f['Tweet']).truncate(after = 199999)\n",
    "\n",
    "f = pd.DataFrame({\"Tweet\": df['Tweet'], \"Status\": text_data['category']})\n",
    "\n",
    "#s = pd.DataFrame({\"Tweet\": df['Tweet'], \"Status\": text_data['category']}).truncate(after = 199999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = s['Tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'].to_csv(r'russian.txt', header=None, index=None, sep=' ', mode='a')\n",
    "content = open('russian.txt', encoding='utf8').read()\n",
    "words = nltk.word_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 Nominees\n",
    "trump = 'realDonaldTrump'\n",
    "clinton = 'HillaryClinton'\n",
    "\n",
    "# Top 6 Prospects\n",
    "biden = 'JoeBiden'\n",
    "booker = 'CoryBooker'\n",
    "sanders = 'BernieSanders'\n",
    "\n",
    "harris = 'KamalaHarris'\n",
    "gillibrand = 'SenGillibrand'\n",
    "warren = 'elizabethforma'\n",
    "\n",
    "# DataFrames for favorites/retweets/tweets for each politician\n",
    "trump_data = twitter.tweets_favorites_retweets(trump)\n",
    "clinton_data = twitter.tweets_favorites_retweets(clinton)\n",
    "\n",
    "biden_data = twitter.tweets_favorites_retweets(biden)\n",
    "booker_data = twitter.tweets_favorites_retweets(booker)\n",
    "sanders_data = twitter.tweets_favorites_retweets(sanders)\n",
    "\n",
    "harris_data = twitter.tweets_favorites_retweets(harris)\n",
    "gillibrand_data = twitter.tweets_favorites_retweets(gillibrand)\n",
    "warren_data = twitter.tweets_favorites_retweets(warren)\n",
    "\n",
    "tweets = trump_data['tweets']\n",
    "tweets = tweets.append(clinton_data['tweets'])\n",
    "tweets = tweets.append(biden_data['tweets'])\n",
    "tweets = tweets.append(booker_data['tweets'])\n",
    "tweets = tweets.append(sanders_data['tweets'])\n",
    "tweets = tweets.append(harris_data['tweets'])\n",
    "tweets = tweets.append(gillibrand_data['tweets'])\n",
    "tweets = tweets.append(warren_data['tweets'])\n",
    "\n",
    "traits = [\"Legitimate\"] * tweets.size\n",
    "out = pd.DataFrame({\"Date\": df['Time Str'], \"Time\": df['Time'], \"Tweet\": df['Tweet']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to first create a list that feature dictionaries from the tokenized tweets will populate\n",
    "#Entire tweets are tokenized into lists of invidiual words, of which each element is associated with value pairs of \"bot\" or \"human\" \n",
    "#to label the category the data belongs to \n",
    "\n",
    "fake_tokens = []\n",
    "real_tokens = []\n",
    "\n",
    "#tokenize each tweet then add to fake_tokens (list with confirmed bot tweet content)\n",
    "for i in range(0, s.size):\n",
    "    item = nltk.word_tokenize(s['Text'][i])\n",
    "    fake_tokens.append(item)\n",
    "    \n",
    "#tokenize each tweet then add to real_tokens (list with confirmed human tweet content)    \n",
    "for i in range(0, out.size):\n",
    "    item = nltk.word_tokenize(out['Tweets'][i])\n",
    "    real_tokens.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function generating feature dictionaries for tweets\n",
    "\n",
    "def create_word_features(words, category):\n",
    "    return dict([(word, category) for word in words])\n",
    "\n",
    "#generated lists, full of dictionaries created by the create_word_features function\n",
    "bot_tweets = [] + create_word_features(fake_tokens, \"bot\")\n",
    "human_tweets = [] + create_word_features(real_tokens, \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ending index for training data\n",
    "train_cutoff = 100001\n",
    "\n",
    "#beginning index for testing data\n",
    "test_begin = 100001\n",
    "\n",
    "#beginning of index for human_tweets\n",
    "human_begin = 10\n",
    "\n",
    "train_set = bot_tweets[:100001]\n",
    "\n",
    "#We increment by 10000 for the bot tweets because each testing set of data is expected to have approximately 1/10 of remaining training data.\n",
    "#We add 1800 human-confirmed tweets to test accuracy for false positives\n",
    "\n",
    "test_set = []\n",
    "\n",
    "#Here we make a loop to create each individual dictionary (for testing)\n",
    "for i in range(1, 11, 1): \n",
    "    test_set[i] = bot_tweets[test_begin:(test_begin) + 10000*i] + human_tweets[human_begin: (human_begin) + 1800]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This checks to see if the file has been run once\n",
    "\n",
    "#pickle will help save the classifier state after the classifier has been trained\n",
    "#pickling is important, as it allows python objecto hierarchies to turn into bitstreams\n",
    "\n",
    "#If the file has not been run, run the code below\n",
    "if run_var = 0:\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    classify_buffer = open('russian_bots_bigram_bayes.pickle', 'wb')\n",
    "    pickle.dump(classifier, classify_buffer)\n",
    "    classify_buffer.close()\n",
    "    run_var += 1\n",
    "    \n",
    "\n",
    "#The alternative is re-opneing the classifier_buffer and updating the classifier    \n",
    "else:\n",
    "    classify_buffer = open('russian_bots_bigram_bayes.pickle', 'rb')\n",
    "    classifier = pickle.load(classify_buffer)\n",
    "    classify_buffer.close()\n",
    "    run_var += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the simple testing suite from nltk to determine how accurate a classifier is\n",
    "def accuracy_tester(classifier, test_set, test_set_index):\n",
    "    print(\"Accuracy of test_set\" + test_set_index + \" is :\", nltk.classify.util.accuracy(classifier, test_set) * 100)\n",
    "\n",
    "#Here we print the accuracy of each of the test sets after running the trained classifier\n",
    "for i in range(1, 11, 1):\n",
    "    accuracy_tester(classifier, test_set, i);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
