{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we import necessary packages \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import functions\n",
    "import twitter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static variable saves how many times the program has been run \n",
    "run_var = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv('tweets.csv')\n",
    "df = pd.DataFrame({\"Tweet\": text_data['text'][0:20], \"Status\": text_data['category'][0:20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sort_values(by=['Time'])\n",
    "df = df.dropna(axis=0, how='all')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Status                                              Tweet\n",
      "0         bot    #ThingsDoneByMistake kissing auntie in the lips\n",
      "1         bot  RT @mc_derpin: #TheOlderWeGet the more pessimi...\n",
      "2         bot  RT @dmataconis: Ready To Feel Like A Failure? ...\n",
      "3         bot    Amen! #blacklivesmatter https://t.co/wGffaOqgzl\n",
      "4         bot  RT @NahBabyNah: Twitchy: Chuck Todd caught out...\n",
      "5         bot  RT @mcicero10: #BernieSanders #Trump people sh...\n",
      "6         bot  RT @ItsJustJaynie: @HillaryClinton The undecid...\n",
      "7         bot                           @TodayCleveland 'no way'\n",
      "8         bot  @NickTomaWBRE Hi, Nick! We're holding a \"Miner...\n",
      "9         bot  What. Is. A. Resolution #My4WordNewYearsResolu...\n",
      "10        bot  Lifetime movie   your pet   psycho neighbor = ...\n",
      "11        bot  RT @Conservatexian: New post: \"UN alarm that m...\n",
      "12        bot  RT @HillaryClinton: This one's for you, Hillar...\n",
      "13        bot  RT @leonpui_: Hillary Clinton, Obama and the D...\n",
      "14        bot  5th grade. When the second plane hit, I though...\n",
      "15        bot  #teapartynews #teaparty #theteaparty #politics...\n",
      "16        bot  #TrumpBecause #DonaldTrump will not be bought!...\n",
      "17        bot  RT @DMashak: #Debates Chris Wallace @FoxNewsSu...\n",
      "18        bot  RT @mikefdupjourney: @she_nutt You're welcome!...\n",
      "19        bot  RT @PrisonPlanet: Hillary's anti-Trump poster ...\n",
      "20        bot                                                NaN\n",
      "21        bot                                                NaN\n",
      "22        bot                                                NaN\n",
      "23        bot                                                NaN\n",
      "24        bot                                                NaN\n",
      "25        bot                                                NaN\n",
      "26        bot                                                NaN\n",
      "27        bot                                                NaN\n",
      "28        bot                                                NaN\n",
      "29        bot                                                NaN\n",
      "...       ...                                                ...\n",
      "203421    bot                                                NaN\n",
      "203422    bot                                                NaN\n",
      "203423    bot                                                NaN\n",
      "203424    bot                                                NaN\n",
      "203425    bot                                                NaN\n",
      "203426    bot                                                NaN\n",
      "203427    bot                                                NaN\n",
      "203428    bot                                                NaN\n",
      "203429    bot                                                NaN\n",
      "203430    bot                                                NaN\n",
      "203431    bot                                                NaN\n",
      "203432    bot                                                NaN\n",
      "203433    bot                                                NaN\n",
      "203434    bot                                                NaN\n",
      "203435    bot                                                NaN\n",
      "203436    bot                                                NaN\n",
      "203437    bot                                                NaN\n",
      "203438    bot                                                NaN\n",
      "203439    bot                                                NaN\n",
      "203440    bot                                                NaN\n",
      "203441    bot                                                NaN\n",
      "203442    bot                                                NaN\n",
      "203443    bot                                                NaN\n",
      "203444    bot                                                NaN\n",
      "203445    bot                                                NaN\n",
      "203446    bot                                                NaN\n",
      "203447    bot                                                NaN\n",
      "203448    bot                                                NaN\n",
      "203449    bot                                                NaN\n",
      "203450    bot                                                NaN\n",
      "\n",
      "[203451 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "f = pd.DataFrame({\"Tweet\": df['Tweet'], \"Status\": text_data['category']})\n",
    "print(f)\n",
    "s = pd.DataFrame({\"Tweet\": df['Tweet'], \"Status\": text_data['category']}).truncate(after = 199999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = s['Tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'].to_csv(r'russian.txt', header=None, index=None, sep=' ', mode='a')\n",
    "content = open('russian.txt', encoding='utf8').read()\n",
    "words = nltk.word_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 Nominees\n",
    "trump = 'realDonaldTrump'\n",
    "clinton = 'HillaryClinton'\n",
    "\n",
    "# Top 6 Prospects\n",
    "biden = 'JoeBiden'\n",
    "booker = 'CoryBooker'\n",
    "sanders = 'BernieSanders'\n",
    "\n",
    "harris = 'KamalaHarris'\n",
    "gillibrand = 'SenGillibrand'\n",
    "warren = 'elizabethforma'\n",
    "\n",
    "# DataFrames for favorites/retweets/tweets for each politician\n",
    "trump_data = twitter.tweets_favorites_retweets(trump)\n",
    "clinton_data = twitter.tweets_favorites_retweets(clinton)\n",
    "\n",
    "biden_data = twitter.tweets_favorites_retweets(biden)\n",
    "booker_data = twitter.tweets_favorites_retweets(booker)\n",
    "sanders_data = twitter.tweets_favorites_retweets(sanders)\n",
    "\n",
    "harris_data = twitter.tweets_favorites_retweets(harris)\n",
    "gillibrand_data = twitter.tweets_favorites_retweets(gillibrand)\n",
    "warren_data = twitter.tweets_favorites_retweets(warren)\n",
    "\n",
    "tweets = trump_data['tweets']\n",
    "tweets = tweets.append(clinton_data['tweets'])\n",
    "tweets = tweets.append(biden_data['tweets'])\n",
    "tweets = tweets.append(booker_data['tweets'])\n",
    "tweets = tweets.append(sanders_data['tweets'])\n",
    "tweets = tweets.append(harris_data['tweets'])\n",
    "tweets = tweets.append(gillibrand_data['tweets'])\n",
    "tweets = tweets.append(warren_data['tweets'])\n",
    "\n",
    "traits = [\"human\"] * tweets.size\n",
    "out = pd.DataFrame({\"Tweet\": df['Tweet'], \"Status\": text_data['category']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-25c7ca45275a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Tweet\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mfake_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "#We need to first create a list of feature dictionaries from the tokenized tweets will populate\n",
    "#Entire tweets are tokenized into lists of invidiual words, of which each element is associated with value pairs of \"bot\" or \"human\" \n",
    "#to label the category the data belongs to \n",
    "\n",
    "fake_tokens = []\n",
    "real_tokens = []\n",
    "\n",
    "#tokenize each tweet then add to fake_tokens (list with confirmed bot tweet content)\n",
    "for i in range(0, s.size):\n",
    "    item = nltk.word_tokenize(s[i][\"Tweet\"])\n",
    "    fake_tokens.append(item)\n",
    "    \n",
    "#tokenize each tweet then add to real_tokens (list with confirmed human tweet content)    \n",
    "for i in range(0, out.size):\n",
    "    item = nltk.word_tokenize(out[i][\"Tweet\"])\n",
    "    real_tokens.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function generating feature dictionaries for tweets\n",
    "\n",
    "def create_word_features(words, category):\n",
    "    return dict([(word, category) for word in words])\n",
    "\n",
    "#generated lists, full of dictionaries created by the create_word_features function\n",
    "bot_tweets = [] + create_word_features(fake_tokens, \"bot\")\n",
    "human_tweets = [] + create_word_features(real_tokens, \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ending index for training data\n",
    "train_cutoff = 100001\n",
    "\n",
    "#beginning index for testing data\n",
    "test_begin = 100001\n",
    "\n",
    "#beginning of index for human_tweets\n",
    "human_begin = 10\n",
    "\n",
    "train_set = bot_tweets[:100001]\n",
    "\n",
    "#We increment by 10000 for the bot tweets because each testing set of data is expected to have approximately 1/10 of remaining training data.\n",
    "#We add 1800 human-confirmed tweets to test accuracy for false positives\n",
    "\n",
    "test_set = []\n",
    "\n",
    "#Here we make a loop to create each individual dictionary (for testing)\n",
    "for i in range(1, 11, 1): \n",
    "    test_set[i] = bot_tweets[test_begin:(test_begin) + 10000*i] + human_tweets[human_begin: (human_begin) + 1800]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This checks to see if the file has been run once\n",
    "\n",
    "#pickle will help save the classifier state after the classifier has been trained\n",
    "#pickling is important, as it allows python objecto hierarchies to turn into bitstreams\n",
    "\n",
    "#If the file has not been run, run the code below\n",
    "if run_var = 0:\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    classify_buffer = open('russian_bots_positive_bayes.pickle', 'wb')\n",
    "    pickle.dump(classifier, classify_buffer)\n",
    "    classify_buffer.close()\n",
    "    run_var += 1\n",
    "    \n",
    "\n",
    "#The alternative is re-opneing the classifier_buffer and updating the classifier    \n",
    "else:\n",
    "    classify_buffer = open('russian_bots_positive_bayes.pickle', 'rb')\n",
    "    classifier = pickle.load(classify_buffer)\n",
    "    classify_buffer.close()\n",
    "    run_var += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the simple testing suite from nltk to determine how accurate a classifier is\n",
    "def accuracy_tester(classifier, test_set, test_set_index):\n",
    "    print(\"Accuracy of test_set\" + test_set_index + \" is :\", nltk.classify.util.accuracy(classifier, test_set) * 100)\n",
    "\n",
    "#Here we print the accuracy of each of the test sets after running the trained classifier\n",
    "for i in range(1, 11, 1):\n",
    "    accuracy_tester(classifier, test_set, i);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
